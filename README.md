# ðŸ›’ Ecommerce Data Pipeline

> End-to-end data pipeline orchestrated with **Apache Airflow**, ingesting synthetic ecommerce data through **AWS S3 + Snowpipe** into a **Snowflake** medallion architecture (Bronze â†’ Silver â†’ Gold), with automated data quality checks via **Great Expectations**.

![Architecture Diagram](Architecture_image.png)

---

## ðŸ“‘ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
- [Running the Pipeline](#running-the-pipeline)
- [Data Layers](#data-layers)
- [Data Quality](#data-quality)
- [Environment Variables](#environment-variables)
- [Author](#author)

---

## Overview

This project simulates a production-grade ecommerce data pipeline:

1. **Synthetic data** (customers, products, sales) is generated daily by an Airflow DAG using `Faker`.
2. Files are uploaded to **AWS S3** into dedicated prefixes (`raw/customers/`, `raw/products/`, `raw/sales/`).
3. **Snowpipe** listens to S3 event notifications and automatically ingests new files into the Snowflake **Bronze** (raw) layer.
4. Airflow then triggers **SQL transformations** to build the **Silver** (cleaned) and **Gold** (aggregated analytics) layers.
5. **Great Expectations** validates data quality at each layer before proceeding.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG        â”‚  (daily schedule)
â”‚  generate_fake_data â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ CSV / JSON
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      S3 Event       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     AWS S3          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Snowpipe (auto)     â”‚
â”‚  raw/customers/     â”‚                     â”‚  loads â†’ Bronze      â”‚
â”‚  raw/products/      â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  raw/sales/         â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚  Snowflake           â”‚
                                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                            â”‚  â”‚  Bronze (raw)  â”‚  â”‚
                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                            â”‚          â”‚ SQL       â”‚
                                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                            â”‚  â”‚ Silver (clean) â”‚  â”‚
                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                            â”‚          â”‚ SQL       â”‚
                                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                            â”‚  â”‚  Gold (aggreg) â”‚  â”‚
                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
Snowflake_Airflow_Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ .airflowignore                  # Tells Airflow which files to ignore
â”‚   â””â”€â”€ eccomerce_pipeline_dag.py       # Main Airflow DAG (full pipeline)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_fake_data.py           # Generates synthetic CSV/JSON data
â”‚   â”œâ”€â”€ from_local_to_s3.py             # Uploads local files to AWS S3
â”‚   â”œâ”€â”€ data_validation.py              # Great Expectations validation suites
â”‚   â””â”€â”€ commands.sh                     # Project setup shell script
â”‚
â”œâ”€â”€ sql_code/
â”‚   â”œâ”€â”€ bronze.sql                      # Warehouse, DB, schema & raw tables DDL
â”‚   â”œâ”€â”€ snowpipe.sql                    # S3 stage + Snowpipe definitions
â”‚   â”œâ”€â”€ silver.sql                      # Silver layer transformations
â”‚   â””â”€â”€ gold.sql                        # Gold layer aggregations
â”‚
â”œâ”€â”€ data/                               # Local data output directory (git-ignored)
â”œâ”€â”€ log/                                # Pipeline logs
â”‚
â”œâ”€â”€ Architecture_image.png              # Architecture diagram
â”œâ”€â”€ Dockerfile                          # Astro Runtime base image
â”œâ”€â”€ airflow_settings.yaml               # Local Airflow connections / variables
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # This file
```

---

## Prerequisites

| Tool | Version |
|------|---------|
| Python | 3.11+ |
| Astro CLI | latest |
| Docker Desktop | 4.x+ |
| AWS Account | S3 + IAM |
| Snowflake Account | any edition |

---

## Setup

### 1. Clone the repository

```bash
git clone https://github.com/MohamedSAIFI0/Snowflake_Airflow_Pipeline.git
cd Snowflake_Airflow_Pipeline
```

### 2. Create and activate a virtual environment (for local scripts)

```bash
python3 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure environment variables

Create a `.env` file at the project root (never commit this file):

```env
# AWS
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-ecommerce-bucket

# Snowflake
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account.region.cloud   # e.g. abc123.eu-west-1.aws
```

### 4. Set up Snowflake (run SQL scripts in order)

```sql
-- In Snowflake worksheet, run in this exact order:
1. sql_code/bronze.sql      -- Creates warehouse, database, schema, raw tables
2. sql_code/snowpipe.sql    -- Creates S3 stage and Snowpipe for each table
3. sql_code/silver.sql      -- Creates cleaned Silver tables
4. sql_code/gold.sql        -- Creates Gold analytics tables
```

### 5. Set up S3 Event Notifications for Snowpipe

In your S3 bucket â†’ **Properties â†’ Event Notifications**, create one notification per prefix:

| Prefix | Event type | Destination |
|--------|-----------|-------------|
| `raw/customers/` | All object create events | Snowflake SNS ARN |
| `raw/products/` | All object create events | Snowflake SNS ARN |
| `raw/sales/` | All object create events | Snowflake SNS ARN |

> The Snowflake SNS ARN is found in your storage integration details: `DESC INTEGRATION my_integration;`

### 6. Configure the Airflow Snowflake connection

Edit `airflow_settings.yaml` for local development:

```yaml
airflow:
  connections:
    - conn_id: snowflake_default
      conn_type: snowflake
      conn_login: your_user
      conn_password: your_password
      conn_extra:
        account: your_account.region.cloud
        warehouse: eccomerce_wh
        database: ecommerce_db
        schema: raw
```

Or via the Airflow UI: **Admin â†’ Connections â†’ Add â†’ Snowflake**.

### 7. Start Airflow with Astro CLI

```bash
astro dev start
```

Access the Airflow UI at `http://localhost:8080` (default credentials: `admin` / `admin`).

---

## Running the Pipeline

### Automatic (Airflow)

The DAG `ecommerce_data_pipeline` runs **daily**. To trigger it manually:

```bash
astro dev run dags trigger ecommerce_data_pipeline
```

Or click **Trigger DAG** in the Airflow UI.

### Manual (standalone scripts)

```bash
# 1. Generate fake data locally
python scripts/generate_fake_data.py

# 2. Upload to S3
python scripts/from_local_to_s3.py

# 3. Run data validation
python scripts/data_validation.py
```

---

## Data Layers

### ðŸ¥‰ Bronze â€” Raw Ingestion (`ecommerce_db.raw`)

| Table | Description |
|-------|-------------|
| `customers` | Raw customer records from CSV |
| `products` | Raw product catalogue from CSV |
| `sales` | Raw sales transactions from JSON |

Data lands here exactly as generated, via Snowpipe from S3.

### ðŸ¥ˆ Silver â€” Cleaned Data (`ecommerce_db.silver`)

| Table | Description |
|-------|-------------|
| `customers_clean` | Deduplicated, trimmed, normalised customers |
| `products_clean` | Deduplicated, positive-price-filtered products |
| `sales_enriched` | Sales joined with clean customers & products, with `total_amount` computed |

### ðŸ¥‡ Gold â€” Analytics (`ecommerce_db.gold`)

| Table | Description |
|-------|-------------|
| `sales_by_country` | Total revenue, sales count and customer count per country |
| `top_products` | Products ranked by total revenue |
| `client_activity` | Per-customer purchase count, total spend and last purchase date |

---

## Data Quality

Validation is handled by **Great Expectations** (v1 Fluent API) in `scripts/data_validation.py` and natively inside the DAG's `data_quality_checks` task.

### Checks performed

| Layer | Check |
|-------|-------|
| Bronze | `customer_id` unique and not null |
| Bronze | `email` matches valid regex |
| Bronze | `price` between 0 and 10 000 |
| Bronze | `category` in allowed set |
| Silver | `sale_id`, `customer_id`, `product_id`, `total_amount` not null |
| Silver | `quantity` between 1 and 100 |
| Silver | `total_amount` â‰¥ 0 |
| Gold | `total_sales` â‰¥ 0 |
| Gold | `number_of_customers` â‰¥ 1 |

The DAG raises a `ValueError` and stops execution if any check fails, preventing bad data from reaching the Gold layer.

---

## Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `AWS_ACCESS_KEY_ID` | AWS IAM access key | âœ… |
| `AWS_SECRET_ACCESS_KEY` | AWS IAM secret key | âœ… |
| `AWS_REGION` | AWS region (e.g. `us-east-1`) | âœ… |
| `S3_BUCKET_NAME` | Target S3 bucket name | âœ… |
| `SNOWFLAKE_USER` | Snowflake username | âœ… |
| `SNOWFLAKE_PASSWORD` | Snowflake password | âœ… |
| `SNOWFLAKE_ACCOUNT` | Snowflake account identifier | âœ… |

---

## Author

**Mohamed SAIFI** â€” [saifimsc@gmail.com](mailto:saifimsc@gmail.com)

---

## References

- [Snowflake Documentation](https://docs.snowflake.com/)
- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Astronomer Astro CLI](https://www.astronomer.io/docs/astro/cli/overview)
- [Great Expectations Documentation](https://docs.greatexpectations.io/)